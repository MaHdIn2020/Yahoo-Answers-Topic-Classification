

## ğŸ“Œ Yahoo Answers Topic Classification

**Machine Learning and Deep Learning Approaches for Multi-Class Text Classification**

---

## ğŸ“– Project Overview

This project focuses on **multi-class text classification** using the **Yahoo Answers Topic Classification dataset**. The goal is to compare traditional machine learning models with deep learning architectures, including recurrent and bidirectional neural networks, to analyze their effectiveness in understanding contextual information in text.

Both **TF-IDF** and **Skip-gram (Word2Vec)** feature representations are explored, followed by extensive **manual hyperparameter tuning** and **library-based tuning** to ensure fair and meaningful comparisons.

---

## ğŸ“‚ Dataset

* **Name:** Yahoo Answers Topic Classification
* **Classes:** 10 topic categories
* **Task:** Assign each questionâ€“answer pair to its correct topic
* **Text Fields Used:** Combined question title, question content, and best answer

---

## ğŸ§¹ Text Preprocessing

The following preprocessing steps were applied:

* Lowercasing text
* Removing HTML tags
* Removing punctuation and special characters
* Tokenization
* Stopword removal
* Lemmatization

These steps help reduce noise and improve feature quality for both classical and neural models.

---

## ğŸ§  Feature Representation

### 1ï¸âƒ£ TF-IDF

Used for:

* Naive Bayes
* Logistic Regression
* Support Vector Machine
* Random Forest
* Deep Neural Network (DNN + SVD)

Dimensionality reduction was performed using **Truncated SVD** before feeding TF-IDF features into neural networks.

---

### 2ï¸âƒ£ Skip-gram (Word2Vec)

* Trained using **Gensim**
* Vector size: 100
* Used for all neural network models
* Both **average Word2Vec** and **sequence-based embeddings** were explored

---

## ğŸ¤– Models Implemented

### ğŸ”¹ Traditional Machine Learning

* Naive Bayes
* Logistic Regression
* Support Vector Machine (SVM)
* Random Forest

Hyperparameters were tuned using **GridSearchCV**.

---

### ğŸ”¹ Neural Network Models

#### TF-IDF Based

* Deep Neural Network (DNN)

#### Skip-gram Based

* DNN (Average Word2Vec)
* Simple RNN
* GRU
* LSTM
* Bidirectional RNN
* Bidirectional GRU
* Bidirectional LSTM

Hyperparameters were tuned using **KerasTuner** and validation performance.

---

## âš™ï¸ Hyperparameter Tuning

* **GridSearchCV** used for all ML models
* **KerasTuner (RandomSearch)** used for neural networks
* Tuned parameters include:

  * Number of units
  * Dropout rate
  * Learning rate
  * Batch size
  * Number of layers

All tuning decisions were guided by **validation accuracy and Macro F1-score**.

---

## ğŸ“Š Evaluation Metrics

* **Accuracy**
* **Macro F1-score**

Macro F1-score was emphasized due to the multi-class nature of the dataset.

---

## ğŸ“ˆ Results Summary

* Deep learning models significantly outperform traditional ML approaches
* Skip-gram embeddings provide richer semantic representations than TF-IDF
* **Bidirectional recurrent architectures** achieve the best performance by capturing context from both past and future tokens
* Bidirectional LSTM is the top-performing model in terms of Macro F1-score

A visual comparison is shown below:

<p align="center">
  <img src="result.png" width="800">
</p>

---

## ğŸ› ï¸ Technologies Used

* Python
* Scikit-learn
* TensorFlow / Keras
* KerasTuner
* Gensim
* NumPy, Pandas, Matplotlib

---

## ğŸ“ Repository Structure

```
Yahoo-Answers-Text-Classification/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Yahoo_Answers_Text_Classification.ipynb
â”‚
â”œâ”€â”€ report/
â”‚   â””â”€â”€ Yahoo_Answers_Text_Classification_Report.pdf
â”‚
â”œâ”€â”€ figures/
â”‚   â””â”€â”€ result.png
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

```

---

## ğŸš€ How to Run

1. Clone the repository:

   ```bash
   git clone https://github.com/your-username/yahoo-answers-classification.git
   ```
2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```
3. Open the notebook and run all cells.

---

## ğŸ‘¤ Author

**Tanjip Surait Mahdin**
Computer Science, BRAC University

---

## ğŸ“Œ Notes

* This project was completed as part of an academic coursework
* Emphasis was placed on fair model comparison and proper hyperparameter tuning
* The notebook is fully reproducible


